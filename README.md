# With Love from KC - Website Development

A comprehensive website for "With Love from KC", a Kansas City business that operates primarily through Instagram. This project implements a complete data acquisition strategy to transform their Instagram presence into a full-featured website.

## ğŸš€ Features

- **Instagram Data Scraping**: Automated scraping of Instagram posts, profile data, and media
- **Data Processing Pipeline**: Image optimization, content categorization, and database integration
- **Modern Web Stack**: Next.js 14, TypeScript, Tailwind CSS, Supabase
- **Admin Dashboard**: Content management and data synchronization tools
- **Responsive Design**: Mobile-first approach with modern UI components

## ğŸ“‹ Prerequisites

- Node.js 18+ 
- npm or yarn
- Supabase account
- Instagram Business Account (for API access)

## ğŸ› ï¸ Installation

1. **Clone and install dependencies**:
   ```bash
   cd with-love-from-kc
   npm install
   ```

2. **Set up environment variables**:
   ```bash
   cp env.example .env.local
   ```
   
   Fill in your environment variables:
   ```env
   NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
   NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
   SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
   INSTAGRAM_ACCESS_TOKEN=your_instagram_token
   INSTAGRAM_USERNAME=withlovefromkc
   ```

3. **Set up Supabase database**:
   ```bash
   # Run the migration to create tables
   # Copy the SQL from supabase/migrations/001_initial_schema.sql
   # and run it in your Supabase SQL editor
   ```

## ğŸ”§ Usage

### Development Server
```bash
npm run dev
```
Open [http://localhost:3000](http://localhost:3000) to view the website.

### Data Scraping
```bash
# Scrape Instagram data
npm run scrape

# Process scraped data
npm run process
```

### Admin Dashboard
Visit [http://localhost:3000/admin](http://localhost:3000/admin) to access the admin dashboard for:
- Instagram data synchronization
- Content management
- Data processing

## ğŸ“Š Data Acquisition Strategy

### 1. Instagram Scraping
- **Profile Data**: Business name, bio, follower count, verification status
- **Posts**: Images, captions, hashtags, engagement metrics
- **Media**: High-resolution images and videos
- **Categorization**: Automatic content classification

### 2. Data Processing
- **Image Optimization**: WebP conversion, responsive sizing
- **Content Classification**: Product, lifestyle, behind-scenes, testimonials
- **Engagement Scoring**: Like/comment analysis for featured content
- **Database Integration**: Structured data storage in Supabase

### 3. Content Management
- **Admin Dashboard**: Manual content curation
- **Automated Sync**: Scheduled data updates
- **Quality Control**: Manual review and approval process

## ğŸ—„ï¸ Database Schema

The project uses Supabase (PostgreSQL) with the following tables:

- **profiles**: Business profile information
- **instagram_posts**: Scraped Instagram content
- **products**: Product listings and details
- **testimonials**: Customer reviews and feedback
- **media_library**: Optimized media assets

## ğŸ¨ Tech Stack

- **Frontend**: Next.js 14, React, TypeScript
- **Styling**: Tailwind CSS
- **Backend**: Supabase (PostgreSQL, Auth, Storage)
- **Scraping**: Puppeteer, Playwright
- **Image Processing**: Sharp
- **Deployment**: Vercel

## ğŸ“ Project Structure

```
with-love-from-kc/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                    # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ admin/              # Admin dashboard
â”‚   â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”‚   â””â”€â”€ page.tsx            # Home page
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ supabase/          # Supabase client setup
â”‚   â””â”€â”€ components/             # React components
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data-scraping/         # Instagram scraping tools
â”‚   â””â”€â”€ data-processing/       # Data processing pipeline
â”œâ”€â”€ supabase/
â”‚   â””â”€â”€ migrations/            # Database schema
â””â”€â”€ public/
    â””â”€â”€ images/                # Optimized media assets
```

## ğŸ”’ Security Considerations

- **Rate Limiting**: Respect Instagram's API limits
- **Data Privacy**: GDPR compliance for EU visitors
- **Authentication**: Secure admin access
- **Input Validation**: Sanitize all user inputs

## ğŸ“ˆ Performance Optimization

- **Image Optimization**: WebP format, responsive images
- **Caching**: API response caching
- **CDN**: Vercel's global CDN
- **Lazy Loading**: Progressive image loading

## ğŸš€ Deployment

### Vercel Deployment
1. Connect your GitHub repository to Vercel
2. Set environment variables in Vercel dashboard
3. Deploy automatically on push to main branch

### Environment Variables for Production
```env
NEXT_PUBLIC_SUPABASE_URL=your_production_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_production_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_production_service_role_key
INSTAGRAM_ACCESS_TOKEN=your_instagram_token
```

## ğŸ“ API Endpoints

- `GET /api/posts` - Fetch Instagram posts
- `GET /api/products` - Fetch product listings
- `POST /api/sync-instagram` - Trigger data synchronization

## ğŸ”„ Data Synchronization

The system supports three sync modes:

1. **Scrape**: Extract new data from Instagram
2. **Process**: Optimize and categorize existing data
3. **Sync**: Complete scrape + process workflow

## ğŸ“Š Monitoring and Analytics

- **Performance**: Vercel Analytics
- **Errors**: Error tracking and logging
- **Usage**: User engagement metrics
- **Data Quality**: Content validation reports

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For questions or issues:
1. Check the documentation
2. Review the admin dashboard
3. Check Supabase logs
4. Contact the development team

---

**Note**: This project implements Instagram data scraping for legitimate business purposes. Ensure compliance with Instagram's Terms of Service and applicable data protection regulations.



